{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission to mars\n",
    "\n",
    "## Web Scraping\n",
    "\n",
    "In this notebook the web scraping of four different web sites about mars is done.\n",
    "\n",
    "The pages that were retreived are\n",
    "\n",
    "1. Web page of Mars mission news (https://mars.nasa.gov/news/)\n",
    "\n",
    "2. Featured image of JPL Mars Space Images (https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars)\n",
    "\n",
    "3. Mars Weather (https://twitter.com/marswxreport?lang=en)\n",
    "\n",
    "4. Mars facts (https://space-facts.com/mars/)\n",
    "\n",
    "5. Mars hemispheres (https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "#Let's call the dependencies\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies to navigate with chrome driver\n",
    "from splinter import Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies to navigate with gecko driver\n",
    "#Try with gecko driver\n",
    "from selenium import webdriver\n",
    "#Create the driver\n",
    "driver = webdriver.Firefox()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasa Mars News\n",
    "\n",
    "In this section the latest news anout Mars mission program is going to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA to Broadcast Mars 2020 Perseverance Launch, Prelaunch Activities\n",
      "Starting July 27, news activities will cover everything from mission engineering and science to returning samples from Mars to, of course, the launch itself.\n",
      "--------\n",
      "The Launch Is Approaching for NASA's Next Mars Rover, Perseverance\n",
      "The Red Planet's surface has been visited by eight NASA spacecraft. The ninth will be the first that includes a roundtrip ticket in its flight plan.\n",
      "--------\n",
      "NASA to Hold Mars 2020 Perseverance Rover Launch Briefing\n",
      "Learn more about the agency's next Red Planet mission during a live event on June 17.\n",
      "--------\n",
      "Alabama High School Student Names NASA's Mars Helicopter\n",
      "Vaneeza Rupani's essay was chosen as the name for the small spacecraft, which will mark NASA's first attempt at powered flight on another planet.\n",
      "--------\n",
      "Mars Helicopter Attached to NASA's Perseverance Rover\n",
      "The team also fueled the rover's sky crane to get ready for this summer's history-making launch.\n",
      "--------\n",
      "NASA's Perseverance Mars Rover Gets Its Wheels and Air Brakes\n",
      "After the rover was shipped from JPL to Kennedy Space Center, the team is getting closer to finalizing the spacecraft for launch later this summer.\n",
      "--------\n",
      "Some mars news: NASA to Broadcast Mars 2020 Perseverance Launch, Prelaunch Activities , Starting July 27, news activities will cover everything from mission engineering and science to returning samples from Mars to, of course, the launch itself.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve page with the requests module\n",
    "# URL of page to be scraped\n",
    "url_news = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response_news = requests.get(url_news)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup_news = BeautifulSoup(response_news.text, 'lxml')\n",
    "\n",
    "#Extract the div tag with class slide\n",
    "results_news = soup_news.find_all('div', class_='slide')\n",
    "\n",
    "#Lets extract the title and the description paragraph, store it in a dictionaty and append the dict to a list\n",
    "mars_news = []\n",
    "\n",
    "for result in results_news:\n",
    "    #Scrape for the title\n",
    "    title = result.find(\"div\", class_=\"content_title\").a.get_text(strip=True)\n",
    "    print(title)\n",
    "    #Scrape for the paragraph text\n",
    "    paragraph =result.find(\"div\", class_=\"rollover_description_inner\").get_text(strip=True)\n",
    "    print(paragraph)\n",
    "    print(\"--------\")\n",
    "    #Lets create a dictionary\n",
    "    news={\n",
    "        \"news_title\": title,\n",
    "        \"news_p\": paragraph\n",
    "    }\n",
    "    #Lets append the dictionary\n",
    "    mars_news.append(news)\n",
    "    \n",
    "#Let's extract the first dictionary values in the first item of the list\n",
    "#Lets extract the first one\n",
    "#Variable that stores the title of the first news\n",
    "news_title = list(mars_news[0].values())[0]\n",
    "#Variable that stores the paragraph text of the first news\n",
    "news_p = list(mars_news[0].values())[1]\n",
    "\n",
    "#Print them\n",
    "print(f\"Some mars news: {news_title} , {news_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured image\n",
    "\n",
    "In this section the web page of the NASA's Jet Propusion Laboratory is scraped to find the featured image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spaceimages/images/largesize/PIA24156_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24155_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24099_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24098_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24152_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24149_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24148_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24147_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24146_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24151_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24145_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24144_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24143_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24142_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24141_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24125_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24091_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24040_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24124_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24123_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24122_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24121_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24120_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24119_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24118_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24117_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24096_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24116_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24115_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24114_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24113_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24112_hires.jpg\n",
      "----\n",
      "NO URL\n",
      "NO URL\n",
      "NO URL\n",
      "NO URL\n",
      "NO URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:493: FutureWarning: browser.find_link_by_partial_text is deprecated. Use browser.links.find_by_partial_text instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spaceimages/images/largesize/PIA24156_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24155_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24099_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24098_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24152_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24149_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24148_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24147_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24146_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24151_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24145_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24144_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24143_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24142_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24141_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24125_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24091_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24040_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24124_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24123_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24122_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24121_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24120_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24119_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24118_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24117_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24096_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24116_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24115_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24114_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24113_hires.jpg\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24112_hires.jpg\n",
      "----\n",
      "NO URL\n",
      "NO URL\n",
      "NO URL\n",
      "NO URL\n",
      "NO URL\n",
      "The final url is https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA24156_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "#Lets do the splinter, declare executable path\n",
    "from splinter import Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#Save the url\n",
    "url_image = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "#Navigate  through chrome driver the website\n",
    "browser.visit(url_image)\n",
    "\n",
    "#Create a string with the Measing part of the the url\n",
    "first_url =\"https://www.jpl.nasa.gov\"\n",
    "\n",
    "#Create an empty list to store the url\n",
    "images_url = []\n",
    "\n",
    "#Iterate throug all the pages\n",
    "for x in range(2):\n",
    "    #Html object\n",
    "    html= browser.html\n",
    "    #Parse HTML  beautiful soup\n",
    "    soup_image = BeautifulSoup(html, 'html.parser')\n",
    "    #Retrieve all elements that contain image information\n",
    "    results_image = soup_image.find_all('li', class_=\"slide\")\n",
    "    #Use beautiful soup's find method to navigate and retrieve attributes\n",
    "    #title and URL\n",
    "    for result in results_image:\n",
    "        try:\n",
    "            anchor = result.find('a', class_=\"fancybox\")[\"data-fancybox-href\"]\n",
    "            c_url = f\"{first_url}{anchor}\"\n",
    "            print(anchor)\n",
    "            print(\"----\")\n",
    "            images_url.append(c_url)\n",
    "        except:\n",
    "            print(\"NO URL\")\n",
    "    \n",
    "    #Click the next button\n",
    "    try:\n",
    "        browser.click_link_by_partial_text('MORE')\n",
    "    except:\n",
    "        print(\"Scraping complete\")\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#Extraxt the fiveth image\n",
    "featured_image_url = images_url[0]\n",
    "\n",
    "#PRINT IT\n",
    "print(f\"The final url is {featured_image_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars weather\n",
    "In this section it is scraped the Mars Weather twitter account in order to find the latest Mars weather tweet from the page. We save the tweet text for the weather report as a variable called mars_weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stablish the twitter url\n",
    "url_weather ='https://twitter.com/marswxreport?lang=en'\n",
    "#Navigate throug the web page with gecko driver\n",
    "driver.get(url_weather)\n",
    "#find the weathet twit by class name\n",
    "time.sleep(2)\n",
    "content = driver.find_element_by_class_name('css-1dbjc4n')\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the position of the word \"InSight\" from the last query\n",
    "start = content.text.find(\"InSight\")\n",
    "#extract the position of the word \"hPa\" from the last query\n",
    "end = content.text.find(\"hPa\")\n",
    "#Convert to text the query\n",
    "result_weather = content.text\n",
    "#Save the result in mars weather\n",
    "mars_weather=result_weather[start:end+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 676 (2020-10-21) low -96.9ºC (-142.4ºF) high -16.5ºC (2.3ºF)\n",
      "winds from the W at 8.9 m/s (19.8 mph) gusting to 26.9 m/s (60.2 mph)\n",
      "pressure at 7.50 hPa\n"
     ]
    }
   ],
   "source": [
    "#Lets print the final result\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts\n",
    "\n",
    "In this section a table that contains some facts about Mars is done. This table is converted into an html table with pandas.\n",
    "\n",
    "The web page is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Mars fact                          Value\n",
      "0  Equatorial Diameter:                       6,792 km\n",
      "1       Polar Diameter:                       6,752 km\n",
      "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
      "3                Moons:            2 (Phobos & Deimos)\n",
      "4       Orbit Distance:       227,943,824 km (1.38 AU)\n",
      "5         Orbit Period:           687 days (1.9 years)\n",
      "6  Surface Temperature:                   -87 to -5 °C\n",
      "7         First Record:              2nd millennium BC\n",
      "8          Recorded By:           Egyptian astronomers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Mars fact</th>\\n      <th>Value</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>Equatorial Diameter:</td>\\n      <td>6,792 km</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>Polar Diameter:</td>\\n      <td>6,752 km</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>Mass:</td>\\n      <td>6.39 × 10^23 kg (0.11 Earths)</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>Moons:</td>\\n      <td>2 (Phobos &amp; Deimos)</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>Orbit Distance:</td>\\n      <td>227,943,824 km (1.38 AU)</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>Orbit Period:</td>\\n      <td>687 days (1.9 years)</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>Surface Temperature:</td>\\n      <td>-87 to -5 °C</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>First Record:</td>\\n      <td>2nd millennium BC</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>Recorded By:</td>\\n      <td>Egyptian astronomers</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets get the url\n",
    "url_mt = 'https://space-facts.com/mars/'\n",
    "#Make pandas read the tables in the web page\n",
    "tables = pd.read_html(url_mt)\n",
    "#Add the column names\n",
    "df = tables[0]\n",
    "df.columns = ['Mars fact', 'Value']\n",
    "print(df)\n",
    "#Convert into a html table\n",
    "html_table = df.to_html()\n",
    "html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars hemispheres\n",
    "\n",
    "In this section it is done the web scraping of the USGS Atrogeology webpage. This is done in order to find high resolution images for each of Mar's hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spaceimages/images/largesize/PIA24112_hires.jpg\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24112_hires.jpg\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24112_hires.jpg\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced\n",
      "----\n",
      "/spaceimages/images/largesize/PIA24112_hires.jpg\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced\n",
      "----\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced\n",
      "[{'title': 'Cerberus Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]\n"
     ]
    }
   ],
   "source": [
    "# URL of page to be scraped\n",
    "url_hem = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response_hem = requests.get(url_hem)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup_hem = BeautifulSoup(response_hem.text, 'lxml')\n",
    "\n",
    "#Lets extrac the div tag with class item \n",
    "results_hem = soup_hem.find_all('a', class_=\"itemLink product-item\")\n",
    "\n",
    "principal_url = []\n",
    "first_hem = \"https://astrogeology.usgs.gov\"\n",
    "#Lets extract the url\n",
    "for result in results_hem:\n",
    "    anchor_hem = result[\"href\"]\n",
    "    hem_url = f\"{first_hem}{anchor_hem}\"\n",
    "    print(anchor)\n",
    "    print(hem_url)\n",
    "    print(\"----\")\n",
    "    principal_url.append(hem_url)\n",
    "    \n",
    "\n",
    "#lets do the splinter inside a for loop\n",
    "#Create an empty list\n",
    "\n",
    "hem_list=[]\n",
    "\n",
    "for addres in principal_url:\n",
    "    aux_url = addres\n",
    "    print(aux_url)\n",
    "    # Retrieve page with the requests module\n",
    "    aux_response = requests.get(aux_url)\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    aux_soup = BeautifulSoup(aux_response.text, 'lxml')\n",
    "    #Lets extract the title\n",
    "    header = aux_soup.find_all('h2', class_=\"title\")\n",
    "    title = header[0].text\n",
    "    #Lets extract the url\n",
    "    downloads = aux_soup.find_all('div',class_= 'downloads')\n",
    "    link = downloads[0].a['href']\n",
    "    #Create a dictionary\n",
    "    hem_dict = {\"title\": title, \"img_url\":link}\n",
    "    #Add to the list\n",
    "    hem_list.append(hem_dict)\n",
    "    \n",
    "print(hem_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
